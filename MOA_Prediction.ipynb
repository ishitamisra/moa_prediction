{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "iq6Q7eRN0FA1"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false,
        "id": "miuVjeX80FA1",
        "outputId": "477de6be-c0c6-4465-955c-e31221cabb51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "from copy import deepcopy as dp\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def norm_fit(df_1,saveM = True, sc_name = 'zsco'):\n",
        "    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n",
        "    ss_1_dic = {'zsco':StandardScaler(),\n",
        "                'mima':MinMaxScaler(),\n",
        "                'maxb':MaxAbsScaler(),\n",
        "                'robu':RobustScaler(),\n",
        "                'norm':Normalizer(),\n",
        "                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n",
        "                'powe':PowerTransformer()}\n",
        "    ss_1 = ss_1_dic[sc_name]\n",
        "    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n",
        "    if saveM == False:\n",
        "        return(df_2)\n",
        "    else:\n",
        "        return(df_2,ss_1)\n",
        "\n",
        "def norm_tra(df_1,ss_x):\n",
        "    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n",
        "    return(df_2)\n",
        "\n",
        "def g_table(list1):\n",
        "    table_dic = {}\n",
        "    for i in list1:\n",
        "        if i not in table_dic.keys():\n",
        "            table_dic[i] = 1\n",
        "        else:\n",
        "            table_dic[i] += 1\n",
        "    return(table_dic)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed=42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'iterstrat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c650f21740d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/iterative-stratification/iterative-stratification-master'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0miterstrat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml_stratifiers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultilabelStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'iterstrat'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4870lpPd0FA2",
        "outputId": "0bddd742-45c7-4a22-be15-256342b57cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "SEED = [0, 1, 2, 3 ,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "input_dir = '../input/lish-moa/'\n",
        "\n",
        "sc_dic = {}\n",
        "feat_dic = {}\n",
        "train_features = pd.read_csv(input_dir+'train_features.csv')\n",
        "train_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\n",
        "train_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\n",
        "test_features = pd.read_csv(input_dir+'test_features.csv')\n",
        "sample_submission = pd.read_csv(input_dir+'sample_submission.csv')\n",
        "train_drug = pd.read_csv(input_dir+'train_drug.csv')\n",
        "\n",
        "target_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
        "target_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n",
        "\n",
        "######## non-score ########\n",
        "nonctr_id = train_features.loc[train_features['cp_type']!='ctl_vehicle','sig_id'].tolist()\n",
        "tmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]\n",
        "mat_cor = pd.DataFrame(np.corrcoef(train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n",
        "                      train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T))\n",
        "mat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]\n",
        "mat_cor2.index = target_nonsc_cols\n",
        "mat_cor2.columns = target_cols\n",
        "mat_cor2 = mat_cor2.dropna()\n",
        "mat_cor2_max = mat_cor2.abs().max(axis = 1)\n",
        "\n",
        "q_n_cut = 0.9\n",
        "target_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()\n",
        "print(len(target_nonsc_cols2))\n",
        "\n",
        "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
        "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
        "feat_dic['gene'] = GENES\n",
        "feat_dic['cell'] = CELLS\n",
        "\n",
        "# sample norm\n",
        "q2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
        "q7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
        "qmean = (q2+q7)/2\n",
        "train_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\n",
        "q2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
        "q7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
        "qmean = (q2+q7)/2\n",
        "test_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n",
        "\n",
        "q2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
        "q7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
        "qmean = (q2+q7)/2\n",
        "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\n",
        "qmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
        "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
        "\n",
        "q2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
        "q7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
        "qmean = (q2+q7)/2\n",
        "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\n",
        "qmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
        "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
        "\n",
        "# remove ctl\n",
        "train = train_features.merge(train_targets_scored, on='sig_id')\n",
        "train = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')\n",
        "\n",
        "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
        "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
        "\n",
        "target = train[['sig_id']+target_cols]\n",
        "target_ns = train[['sig_id']+target_nonsc_cols2]\n",
        "\n",
        "train0 = train.drop('cp_type', axis=1)\n",
        "test = test.drop('cp_type', axis=1)\n",
        "\n",
        "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
        "\n",
        "# drug ids\n",
        "tar_sig = target['sig_id'].tolist()\n",
        "train_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\n",
        "target = target.merge(train_drug, on='sig_id', how='left')\n",
        "\n",
        "# LOCATE DRUGS\n",
        "vc = train_drug.drug_id.value_counts()\n",
        "vc1 = vc.loc[vc <= 19].index\n",
        "vc2 = vc.loc[vc > 19].index\n",
        "\n",
        "feature_cols = []\n",
        "for key_i in feat_dic.keys():\n",
        "    value_i = feat_dic[key_i]\n",
        "    print(key_i,len(value_i))\n",
        "    feature_cols += value_i\n",
        "len(feature_cols)\n",
        "feature_cols0 = dp(feature_cols)\n",
        "\n",
        "oof = np.zeros((len(train), len(target_cols)))\n",
        "predictions = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "# Averaging on multiple SEEDS\n",
        "for seed in SEED:\n",
        "\n",
        "    seed_everything(seed=seed)\n",
        "    folds = train0.copy()\n",
        "    feature_cols = dp(feature_cols0)\n",
        "\n",
        "    # kfold - leave drug out\n",
        "    target2 = target.copy()\n",
        "    dct1 = {}; dct2 = {}\n",
        "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
        "    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
        "    tmp_idx = tmp.index.tolist()\n",
        "    tmp_idx.sort()\n",
        "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
        "    tmp = tmp.loc[tmp_idx2]\n",
        "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
        "        dd = {k:fold for k in tmp.index[idxV].values}\n",
        "        dct1.update(dd)\n",
        "\n",
        "    # STRATIFY DRUGS MORE THAN 19X\n",
        "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
        "    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n",
        "    tmp_idx = tmp.index.tolist()\n",
        "    tmp_idx.sort()\n",
        "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
        "    tmp = tmp.loc[tmp_idx2]\n",
        "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
        "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
        "        dct2.update(dd)\n",
        "\n",
        "    target2['kfold'] = target2.drug_id.map(dct1)\n",
        "    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n",
        "    target2.kfold = target2.kfold.astype(int)\n",
        "\n",
        "    folds['kfold'] = target2['kfold'].copy()\n",
        "\n",
        "    train = folds.copy()\n",
        "    test_ = test.copy()\n",
        "\n",
        "    # HyperParameters\n",
        "    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    EPOCHS = 25\n",
        "    BATCH_SIZE = 128\n",
        "    LEARNING_RATE = 1e-3\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    NFOLDS = 5\n",
        "    EARLY_STOPPING_STEPS = 10\n",
        "    EARLY_STOP = False\n",
        "\n",
        "    n_comp1 = 50\n",
        "    n_comp2 = 15\n",
        "\n",
        "    num_features=len(feature_cols) + n_comp1 + n_comp2\n",
        "    num_targets=len(target_cols)\n",
        "    num_targets_0=len(target_nonsc_cols2)\n",
        "    hidden_size=4096\n",
        "\n",
        "    tar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n",
        "    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
        "    tar_weight0_min = dp(np.min(tar_weight0))\n",
        "    tar_weight = tar_weight0_min/tar_weight0\n",
        "    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n",
        "    from torch.nn.modules.loss import _WeightedLoss\n",
        "    class SmoothBCEwLogits(_WeightedLoss):\n",
        "        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
        "            super().__init__(weight=weight, reduction=reduction)\n",
        "            self.smoothing = smoothing\n",
        "            self.weight = weight\n",
        "            self.reduction = reduction\n",
        "\n",
        "        @staticmethod\n",
        "        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
        "            assert 0 <= smoothing < 1\n",
        "            with torch.no_grad():\n",
        "                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
        "            return targets\n",
        "\n",
        "        def forward(self, inputs, targets):\n",
        "            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
        "                self.smoothing)\n",
        "            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
        "                                                      pos_weight = pos_weight)\n",
        "\n",
        "            if  self.reduction == 'sum':\n",
        "                loss = loss.sum()\n",
        "            elif  self.reduction == 'mean':\n",
        "                loss = loss.mean()\n",
        "\n",
        "            return loss\n",
        "\n",
        "    class TrainDataset:\n",
        "        def __init__(self, features, targets):\n",
        "            self.features = features\n",
        "            self.targets = targets\n",
        "\n",
        "        def __len__(self):\n",
        "            return (self.features.shape[0])\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            dct = {\n",
        "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
        "                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n",
        "            }\n",
        "            return dct\n",
        "\n",
        "    class TestDataset:\n",
        "        def __init__(self, features):\n",
        "            self.features = features\n",
        "\n",
        "        def __len__(self):\n",
        "            return (self.features.shape[0])\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            dct = {\n",
        "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
        "            }\n",
        "            return dct\n",
        "\n",
        "\n",
        "    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
        "        model.train()\n",
        "        final_loss = 0\n",
        "\n",
        "        for data in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = data['x'].to(device), data['y'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            final_loss += loss.item()\n",
        "\n",
        "        final_loss /= len(dataloader)\n",
        "\n",
        "        return final_loss\n",
        "\n",
        "\n",
        "    def valid_fn(model, loss_fn, dataloader, device):\n",
        "        model.eval()\n",
        "        final_loss = 0\n",
        "        valid_preds = []\n",
        "\n",
        "        for data in dataloader:\n",
        "            inputs, targets = data['x'].to(device), data['y'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            final_loss += loss.item()\n",
        "            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "\n",
        "        final_loss /= len(dataloader)\n",
        "        valid_preds = np.concatenate(valid_preds)\n",
        "\n",
        "        return final_loss, valid_preds\n",
        "\n",
        "    def inference_fn(model, dataloader, device):\n",
        "        model.eval()\n",
        "        preds = []\n",
        "\n",
        "        for data in dataloader:\n",
        "            inputs = data['x'].to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "\n",
        "            preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(preds)\n",
        "\n",
        "        return preds\n",
        "\n",
        "    class Model(nn.Module):\n",
        "        def __init__(self, num_features, num_targets, hidden_size):\n",
        "            super(Model, self).__init__()\n",
        "            cha_1 = 256\n",
        "            cha_2 = 512\n",
        "            cha_3 = 512\n",
        "\n",
        "            cha_1_reshape = int(hidden_size/cha_1)\n",
        "            cha_po_1 = int(hidden_size/cha_1/2)\n",
        "            cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
        "\n",
        "            self.cha_1 = cha_1\n",
        "            self.cha_2 = cha_2\n",
        "            self.cha_3 = cha_3\n",
        "            self.cha_1_reshape = cha_1_reshape\n",
        "            self.cha_po_1 = cha_po_1\n",
        "            self.cha_po_2 = cha_po_2\n",
        "\n",
        "            self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
        "            self.dropout1 = nn.Dropout(0.1)\n",
        "            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
        "\n",
        "            self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
        "            self.dropout_c1 = nn.Dropout(0.1)\n",
        "            self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
        "\n",
        "            self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
        "\n",
        "            self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
        "            self.dropout_c2 = nn.Dropout(0.1)\n",
        "            self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
        "\n",
        "            self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
        "            self.dropout_c2_1 = nn.Dropout(0.3)\n",
        "            self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
        "\n",
        "            self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
        "            self.dropout_c2_2 = nn.Dropout(0.2)\n",
        "            self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
        "\n",
        "            self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "            self.flt = nn.Flatten()\n",
        "\n",
        "            self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
        "            self.dropout3 = nn.Dropout(0.2)\n",
        "            self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
        "\n",
        "        def forward(self, x):\n",
        "\n",
        "            x = self.batch_norm1(x)\n",
        "            x = self.dropout1(x)\n",
        "            x = F.celu(self.dense1(x), alpha=0.06)\n",
        "\n",
        "            x = x.reshape(x.shape[0],self.cha_1,\n",
        "                          self.cha_1_reshape)\n",
        "\n",
        "            x = self.batch_norm_c1(x)\n",
        "            x = self.dropout_c1(x)\n",
        "            x = F.relu(self.conv1(x))\n",
        "\n",
        "            x = self.ave_po_c1(x)\n",
        "\n",
        "            x = self.batch_norm_c2(x)\n",
        "            x = self.dropout_c2(x)\n",
        "            x = F.relu(self.conv2(x))\n",
        "            x_s = x\n",
        "\n",
        "            x = self.batch_norm_c2_1(x)\n",
        "            x = self.dropout_c2_1(x)\n",
        "            x = F.relu(self.conv2_1(x))\n",
        "\n",
        "            x = self.batch_norm_c2_2(x)\n",
        "            x = self.dropout_c2_2(x)\n",
        "            x = F.relu(self.conv2_2(x))\n",
        "            x =  x * x_s\n",
        "\n",
        "            x = self.max_po_c2(x)\n",
        "\n",
        "            x = self.flt(x)\n",
        "\n",
        "            x = self.batch_norm3(x)\n",
        "            x = self.dropout3(x)\n",
        "            x = self.dense3(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def run_training(fold, seed):\n",
        "\n",
        "        seed_everything(seed)\n",
        "\n",
        "        trn_idx = train[train['kfold'] != fold].index\n",
        "        val_idx = train[train['kfold'] == fold].index\n",
        "\n",
        "        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n",
        "        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n",
        "\n",
        "        x_train, y_train,y_train_ns = train_df[feature_cols], train_df[target_cols].values,train_df[target_nonsc_cols2].values\n",
        "        x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols], valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n",
        "        x_test = test_[feature_cols]\n",
        "\n",
        "        #------------ norm --------------\n",
        "        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n",
        "        col_num.sort()\n",
        "        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n",
        "        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n",
        "        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n",
        "\n",
        "        #------------ pca --------------\n",
        "        def pca_pre(tr,va,te,\n",
        "                    n_comp,feat_raw,feat_new):\n",
        "            pca = PCA(n_components=n_comp, random_state=42)\n",
        "            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n",
        "            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n",
        "            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n",
        "            return(tr2,va2,te2)\n",
        "\n",
        "\n",
        "        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n",
        "        feat_dic['pca_g'] = pca_feat_g\n",
        "        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n",
        "                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n",
        "        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n",
        "        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n",
        "        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n",
        "\n",
        "        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n",
        "        feat_dic['pca_c'] = pca_feat_g\n",
        "        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n",
        "                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n",
        "        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n",
        "        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n",
        "        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n",
        "\n",
        "        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n",
        "\n",
        "        train_dataset = TrainDataset(x_train, y_train_ns)\n",
        "        valid_dataset = TrainDataset(x_valid, y_valid_ns)\n",
        "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        model = Model(\n",
        "            num_features=num_features,\n",
        "            num_targets=num_targets_0,\n",
        "            hidden_size=hidden_size,\n",
        "        )\n",
        "\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5,\n",
        "                                                  max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
        "\n",
        "        loss_tr = nn.BCEWithLogitsLoss()   #SmoothBCEwLogits(smoothing = 0.001)\n",
        "        loss_va = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
        "        early_step = 0\n",
        "\n",
        "        for epoch in range(1):\n",
        "            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
        "            valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n",
        "            print(f\"FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
        "\n",
        "        model.dense3 = nn.utils.weight_norm(nn.Linear(model.cha_po_2, num_targets))\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        train_dataset = TrainDataset(x_train, y_train)\n",
        "        valid_dataset = TrainDataset(x_valid, y_valid)\n",
        "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
        "                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
        "\n",
        "        loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n",
        "        loss_va = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
        "        early_step = 0\n",
        "\n",
        "        oof = np.zeros((len(train), len(target_cols)))\n",
        "        best_loss = np.inf\n",
        "\n",
        "        mod_name = f\"FOLD_mod11_{seed}_{fold}_.pth\"\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "\n",
        "            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
        "            valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n",
        "            print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
        "\n",
        "            if valid_loss < best_loss:\n",
        "\n",
        "                best_loss = valid_loss\n",
        "                oof[val_idx] = valid_preds\n",
        "                torch.save(model.state_dict(), mod_name)\n",
        "\n",
        "            elif(EARLY_STOP == True):\n",
        "\n",
        "                early_step += 1\n",
        "                if (early_step >= early_stopping_steps):\n",
        "                    break\n",
        "\n",
        "        #--------------------- PREDICTION---------------------\n",
        "        testdataset = TestDataset(x_test)\n",
        "        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        model = Model(\n",
        "            num_features=num_features,\n",
        "            num_targets=num_targets,\n",
        "            hidden_size=hidden_size,\n",
        "        )\n",
        "\n",
        "        model.load_state_dict(torch.load(mod_name))\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        predictions = np.zeros((len(test_), len(target_cols)))\n",
        "        predictions = inference_fn(model, testloader, DEVICE)\n",
        "        return oof, predictions\n",
        "\n",
        "    def run_k_fold(NFOLDS, seed):\n",
        "        oof = np.zeros((len(train), len(target_cols)))\n",
        "        predictions = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "        for fold in range(NFOLDS):\n",
        "            oof_, pred_ = run_training(fold, seed)\n",
        "\n",
        "            predictions += pred_ / NFOLDS\n",
        "            oof += oof_\n",
        "\n",
        "        return oof, predictions\n",
        "\n",
        "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
        "    oof += oof_ / len(SEED)\n",
        "    predictions += predictions_ / len(SEED)\n",
        "\n",
        "    oof_tmp = dp(oof)\n",
        "    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n",
        "    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n",
        "\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "print(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n",
        "\n",
        "train0[target_cols] = oof\n",
        "test[target_cols] = predictions\n",
        "\n",
        "### for blend test ###\n",
        "train0.to_csv('train_pred.csv', index=False)\n",
        "### for blend test ###\n",
        "\n",
        "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
        "sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../input/lish-moa/train_features.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-70f8c7a01ded>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msc_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeat_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train_features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_targets_scored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train_targets_scored.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_targets_nonscored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train_targets_nonscored.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/lish-moa/train_features.csv'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CZQhF47z0FA2",
        "outputId": "d93012b0-5a25-4be6-f1dc-6e14501082af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(sc_dic,index=['sc']).T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [sc]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1a83a28-1e1b-412e-8490-786571bea260\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1a83a28-1e1b-412e-8490-786571bea260')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1a83a28-1e1b-412e-8490-786571bea260 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1a83a28-1e1b-412e-8490-786571bea260');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 0,\n  \"fields\": [\n    {\n      \"column\": \"sc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VpS0bGcL0FA2"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}